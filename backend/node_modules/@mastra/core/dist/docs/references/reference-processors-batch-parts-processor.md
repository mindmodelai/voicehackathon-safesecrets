# BatchPartsProcessor

The `BatchPartsProcessor` is an **output processor** that batches multiple stream parts together to reduce the frequency of emissions during streaming. This processor is useful for reducing network overhead, improving user experience by consolidating small text chunks, and optimizing streaming performance by controlling when parts are emitted to the client.

## Usage example

```typescript
import { BatchPartsProcessor } from "@mastra/core/processors";

const processor = new BatchPartsProcessor({
  batchSize: 5,
  maxWaitTime: 100,
  emitOnNonText: true
});
```

## Constructor parameters

**options?:** (`Options`): Configuration options for batching stream parts

### Options

**batchSize?:** (`number`): Number of parts to batch together before emitting

**maxWaitTime?:** (`number`): Maximum time to wait before emitting a batch (in milliseconds). If set, will emit the current batch even if it hasn't reached batchSize

**emitOnNonText?:** (`boolean`): Whether to emit immediately when a non-text part is encountered

## Returns

**id:** (`string`): Processor identifier set to 'batch-parts'

**name?:** (`string`): Optional processor display name

**processOutputStream:** (`(args: { part: ChunkType; streamParts: ChunkType[]; state: Record<string, any>; abort: (reason?: string) => never }) => Promise<ChunkType | null>`): Processes streaming output parts to batch them together

**flush:** (`(state?: BatchPartsState) => ChunkType | null`): Force flush any remaining batched parts when the stream ends

## Extended usage example

```typescript
import { Agent } from "@mastra/core/agent";
import { BatchPartsProcessor } from "@mastra/core/processors";

export const agent = new Agent({
  name: "batched-agent",
  instructions: "You are a helpful assistant",
  model: "openai/gpt-5.1",
  outputProcessors: [
    new BatchPartsProcessor({
      batchSize: 5,
      maxWaitTime: 100,
      emitOnNonText: true
    })
  ]
});
```

## Related

- [Guardrails](https://mastra.ai/docs/agents/guardrails)