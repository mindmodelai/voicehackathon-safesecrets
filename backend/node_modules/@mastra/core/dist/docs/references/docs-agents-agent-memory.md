# Agent memory

Agents use memory to maintain context across interactions. LLMs are stateless and don't retain information between calls, so agents need memory to track message history and recall relevant information.

Mastra agents can be configured to store message history, with optional [working memory](https://mastra.ai/docs/memory/working-memory) to maintain recent context, [semantic recall](https://mastra.ai/docs/memory/semantic-recall) to retrieve past messages based on meaning, or [observational memory](https://mastra.ai/docs/memory/observational-memory) for automatic long-term memory that compresses conversations as they grow.

## When to use memory

Use memory when your agent needs to maintain multi-turn conversations that reference prior exchanges, recall user preferences or facts from earlier in a session, or build context over time within a conversation thread. Skip memory for single-turn requests where each interaction is independent.

## Setting up memory

To enable memory in Mastra, install the `@mastra/memory` package along with a storage provider.

**npm**:

```bash
npm install @mastra/memory@latest @mastra/libsql@latest
```

**pnpm**:

```bash
pnpm add @mastra/memory@latest @mastra/libsql@latest
```

**Yarn**:

```bash
yarn add @mastra/memory@latest @mastra/libsql@latest
```

**Bun**:

```bash
bun add @mastra/memory@latest @mastra/libsql@latest
```

## Storage providers

Memory requires a storage provider to persist message history, including user messages and agent responses. For more details on available providers and how storage works in Mastra, see the [Storage](https://mastra.ai/docs/memory/storage) documentation.

## Configuring memory

1. Enable memory by creating a `Memory` instance and passing it to the agent’s `memory` option.

   ```typescript
   import { Agent } from "@mastra/core/agent";
   import { Memory } from "@mastra/memory";

   export const memoryAgent = new Agent({
     id: 'memory-agent',
     name: 'Memory Agent',
     memory: new Memory({
       options: {
         lastMessages: 20,
       },
     }),
   });
   ```

   > **Info:** Visit [Memory Class](https://mastra.ai/reference/memory/memory-class) for a full list of configuration options.

2. Add a storage provider to your main Mastra instance to enable memory across all configured agents.

   ```typescript
   import { Mastra } from "@mastra/core";
   import { LibSQLStore } from "@mastra/libsql";

   export const mastra = new Mastra({
     storage: new LibSQLStore({
       id: 'mastra-storage',
       url: ":memory:",
     }),
   });
   ```

   > **Info:** Visit [libSQL Storage](https://mastra.ai/reference/storage/libsql) for a full list of configuration options.

Alternatively, add storage directly to an agent’s memory to keep data separate or use different providers per agent.

```typescript
import { Agent } from "@mastra/core/agent";
import { Memory } from "@mastra/memory";
import { LibSQLStore } from "@mastra/libsql";

export const memoryAgent = new Agent({
  id: 'memory-agent',
  name: 'Memory Agent',
  memory: new Memory({
    storage: new LibSQLStore({
      id: 'mastra-storage',
      url: ":memory:",
    }),
  }),
});
```

> **Mastra Cloud Store limitation:** Agent-level storage is not supported when using [Mastra Cloud Store](https://mastra.ai/docs/mastra-cloud/deployment). If you use Mastra Cloud Store, configure storage on the Mastra instance instead. This limitation does not apply if you bring your own database.

## Message history

Include a `memory` object with both `resource` and `thread` to track message history during agent calls.

- `resource`: A stable identifier for the user or entity.
- `thread`: An ID that isolates a specific conversation or session.

These fields tell the agent where to store and retrieve context, enabling persistent, thread-aware memory across a conversation.

```typescript
const response = await memoryAgent.generate(
  "Remember my favorite color is blue.",
  {
    memory: {
      resource: "user-123",
      thread: "conversation-123",
    },
  },
);
```

To recall information stored in memory, call the agent with the same `resource` and `thread` values used in the original conversation.

```typescript
const response = await memoryAgent.generate("What's my favorite color?", {
  memory: {
    resource: "user-123",
    thread: "conversation-123",
  },
});
```

> **Warning:** Each thread has an owner (`resourceId`) that cannot be changed after creation. Avoid reusing the same thread ID for threads with different owners, as this will cause errors when querying.

To learn more about memory see the [Memory](https://mastra.ai/docs/memory/overview) documentation.

## Observational Memory

For long-running conversations, raw message history grows until it fills the context window, degrading agent performance. [Observational Memory](https://mastra.ai/docs/memory/observational-memory) solves this by running background agents that compress old messages into dense observations, keeping the context window small while preserving long-term memory.

```typescript
import { Agent } from "@mastra/core/agent";
import { Memory } from "@mastra/memory";

export const memoryAgent = new Agent({
  id: 'memory-agent',
  name: 'Memory Agent',
  memory: new Memory({
    options: {
      observationalMemory: true,
    },
  }),
});
```

Setting `observationalMemory: true` uses `google/gemini-2.5-flash` as the default model for the Observer and Reflector. To use a different model or customize thresholds, pass a config object:

```typescript
import { Agent } from "@mastra/core/agent";
import { Memory } from "@mastra/memory";

export const memoryAgent = new Agent({
  id: 'memory-agent',
  name: 'Memory Agent',
  memory: new Memory({
    options: {
      observationalMemory: {
        model: "deepseek/deepseek-reasoner",
        observation: {
          messageTokens: 20_000,
        },
      },
    },
  }),
});
```

> **Info:** See [Observational Memory](https://mastra.ai/docs/memory/observational-memory) for details on how observations and reflections work, and [the reference](https://mastra.ai/reference/memory/observational-memory) for all configuration options.

## Using `RequestContext`

Use [RequestContext](https://mastra.ai/docs/server/request-context) to access request-specific values. This lets you conditionally select different memory or storage configurations based on the context of the request.

```typescript
export type UserTier = {
  "user-tier": "enterprise" | "pro";
};

const premiumMemory = new Memory();

const standardMemory = new Memory();

export const memoryAgent = new Agent({
  id: 'memory-agent',
  name: 'Memory Agent',
  memory: ({ requestContext }) => {
    const userTier = requestContext.get("user-tier") as UserTier["user-tier"];

    return userTier === "enterprise" ? premiumMemory : standardMemory;
  },
});
```

> **Info:** Visit [Request Context](https://mastra.ai/docs/server/request-context) for more information.

## Related

- [Observational Memory](https://mastra.ai/docs/memory/observational-memory)
- [Working Memory](https://mastra.ai/docs/memory/working-memory)
- [Semantic Recall](https://mastra.ai/docs/memory/semantic-recall)
- [Storage](https://mastra.ai/docs/memory/storage)
- [Request Context](https://mastra.ai/docs/server/request-context)